{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6858e8a3",
   "metadata": {},
   "source": [
    "# **Feature Engineering**\n",
    "\n",
    "Feature engineering is the process of transforming raw data into meaningful inputs (features) that improve a machine learning model’s performance. It’s one of the most important steps in building a successful ML model.  \n",
    "\n",
    "Think of it like cooking:  \n",
    "- **Raw ingredients** = Raw data  \n",
    "- **Chopping, mixing, and seasoning** = Feature engineering  \n",
    "- **Delicious dish** = A well-performing ML model  \n",
    "\n",
    "---\n",
    "\n",
    "### **Why is Feature Engineering Important?**  \n",
    "A machine learning model is only as good as the data it learns from. Even if you have a powerful model, poor-quality features can lead to bad predictions. Good features help the model understand patterns better, leading to improved accuracy.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Feature Transformation**\n",
    "\n",
    "- #### **Handling Missing Values (Imputation)**\n",
    "\n",
    "    - Replacing missing values with a meaningful estimate such as mean, median, or mode.\n",
    "    - Example: Filling missing ages in a dataset with the average age.\n",
    "\n",
    "- #### **Handling Categorical Variables**\n",
    "\n",
    "    - Converting categorical variables (e.g., dog, cat, sheep) into numerical values.\n",
    "    - **One-Hot Encoding**: Representing categories as binary columns.\n",
    "    - Example:\n",
    "      - Dog → [1, 0, 0]\n",
    "      - Cat → [0, 1, 0]\n",
    "      - Sheep → [0, 0, 1]\n",
    "    - **Ordinal Encoding**: Assigning ordered numerical values to categories.\n",
    "    - Example: Small (1), Medium (2), Large (3)\n",
    "\n",
    "- #### **Binning Continuous Features**\n",
    "\n",
    "    - Converting numerical features into categorical ranges.\n",
    "    - Example: Age → {0-12: Child, 13-19: Teen, 20+: Adult}\n",
    "\n",
    "- #### **Outlier Detection and Handling**\n",
    "\n",
    "    - Detecting extreme values that can distort model training.\n",
    "    - Methods: Z-score, IQR method, Winsorization, Clipping.\n",
    "\n",
    "- #### **Scaling & Normalization**\n",
    "\n",
    "    - Different numerical features have different scales (e.g., Age vs. Salary).\n",
    "    - **MinMax Scaling**: Rescales data to a fixed range, usually [0,1].\n",
    "    - **Standardization**: Converts data to have zero mean and unit variance.\n",
    "    - **Normalization**: Converts values to a range between -1 and 1.\n",
    "    - **Mean Absolute Scaling**: Scaling based on mean absolute deviation.\n",
    "    - Example: Converting prices from dollars to a normalized scale between 0 and 1.\n",
    "\n",
    "---\n",
    "\n",
    "## **Feature Construction**\n",
    "\n",
    "- #### **Feature Splitting**\n",
    "\n",
    "    - Splitting an existing feature into multiple meaningful features.\n",
    "    - Example: Splitting a full name column into First Name and Last Name.\n",
    "\n",
    "- #### **Feature Grouping**\n",
    "\n",
    "    - Combining related features to create more meaningful representations.\n",
    "    - Example: Grouping similar product categories together in a dataset.\n",
    "\n",
    "- #### **Creating New Features from Existing Ones**\n",
    "\n",
    "    - Example: Instead of using \"Date of Birth,\" deriving \"Age\".\n",
    "    - **Titanic Dataset Example**:\n",
    "      - Given columns: `SibSp` (Number of siblings/spouses) and `Parch` (Number of parents/children).\n",
    "      - New Feature: `Family Size` = `SibSp + Parch`.\n",
    "      - Further Classification:\n",
    "        - `0: Alone`\n",
    "        - `1-4: Small Family`\n",
    "        - `>4: Large Family`\n",
    "\n",
    "- #### **Feature Interaction**\n",
    "\n",
    "    - Creating new features by combining two or more existing features.\n",
    "    - Example: In an e-commerce dataset, multiplying `price` and `quantity_sold` to create a `total_revenue` feature.\n",
    "\n",
    "- #### **Polynomial Features**\n",
    "\n",
    "    - Creating higher-order features to capture non-linear relationships.\n",
    "    - Example: Given `feature X`, generating `X²`, `X³`, etc., to enhance model expressiveness.\n",
    "\n",
    "- #### **Time-Based Feature Engineering**\n",
    "\n",
    "    - Extracting useful features from time-related data.\n",
    "    - Example: Extracting `day_of_week`, `month`, or `hour` from a timestamp for a sales forecasting model.\n",
    "\n",
    "---\n",
    "\n",
    "## **Feature Selection**\n",
    "\n",
    "Selecting the most relevant features to improve model efficiency and performance.\n",
    "\n",
    "- #### **Methods of Feature Selection**\n",
    "\n",
    "    - **Forward Selection**: Iteratively adding the best-performing feature.\n",
    "    - **Backward Elimination**: Removing the least important feature one at a time.\n",
    "    - **Example**: In house price prediction, keeping features like `size` and `location` while removing `house color`.\n",
    "\n",
    "- #### **Example: MNIST Dataset**\n",
    "\n",
    "    - 50,000 images of handwritten digits.\n",
    "    - Low-resolution images: 28 × 28 = 784 features.\n",
    "    - Using **Feature Selection**, only selecting the central pixels that contribute to digit formation, ignoring empty spaces.\n",
    "\n",
    "---\n",
    "\n",
    "## **Feature Extraction**\n",
    "\n",
    "Reducing the number of features while preserving relevant information.\n",
    "\n",
    "- #### **Example: Principal Component Analysis (PCA)**\n",
    "\n",
    "    - Reduces dimensionality while retaining maximum variance.\n",
    "    - Example: Converting 100 features into 10 principal components.\n",
    "\n",
    "- #### **Example: Real Estate Dataset**\n",
    "\n",
    "    - Given features: `Rooms`, `Washrooms`, `Price`.\n",
    "    - Instead of using `Rooms` and `Washrooms` separately, merging them into `Total Square Feet Area`, and then using it as a feature.\n",
    "\n",
    "- #### **Other Dimensionality Reduction Techniques**\n",
    "\n",
    "    - **Linear Discriminant Analysis (LDA)**\n",
    "    - **Autoencoders** (Neural Network-based dimensionality reduction)\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Takeaways**\n",
    "\n",
    "- ✅ Feature Engineering improves model accuracy and efficiency.\n",
    "- ✅ Transforming, selecting, and extracting features are crucial steps.\n",
    "- ✅ Choosing the right features helps in reducing computational costs and preventing overfitting.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
